\section{Tests we performed}
For the testing of our agent we actually employed an agile development technique. This allowed us to get fast results, and use them right away to implement changes.
\\\\
The real testing was done in the domain described in paragraph~\ref{sec:domain}. Our test case consisted of three parts for each run. We started the collaborative scenario (profiles 1, 2, and 3), the moderate scenario (profiles 4, 5, and 6), and the competitive scenario (profiles 7, 8, and 9), and compared the results to each previous run.
\\
Values from the output we considered were:
\begin{itemize}
\item Whether the graph looked like we expected;
\item Whether or not there was an agreement;
\item How soon the agreement was made;
\item How far the agreement product was from the nash line;
\item Distance to pareto;
\item Distance to Nash (which appeared to be different from the difference between the product and the nash line).
\end{itemize}

When this result was to our liking (i.e. when the solution was still pareto and the distance was closer to Nash), we used to previously implemented change, and went on to the next improvement. When it was not to our liking, or when there was an error, we tried to correct it, or we reviewed our thoughts about the usefulness of this feature.
\\\\
In our agent, there are multiple values and thresholds that we can adjust when competing with a different negotiation agent. For example, the amount of rounds that is needed for us to build a reliable Opponent Model. In the tests that we did, there was never much difference when we changed these values, but this is because we were only able to test against ourselves. Secondly, our agent is based on a utility model of the opponent, and not based on the bids themselves. This makes it hard to get varying results, when all we can test against is ourselves. 

\subsection{Estimated preference profiles}

After dialing in our agent, we decided to compare the actual preference profiles with the ones we estimate for an opponent.
The estimated values we obtained can be found in section \ref{sec:attached_estimated_preference_profiles}. Note: we were unable to create reliable preference profiles for profile 1, 2, 3 and 4, because these accepted the offers before the 20 round mark (these tests were done with the threshold of a long game at 20 rounds, it was only after this we have changed the threshold to 10). Also,the estimated profile 5 and 6 are based on only 20 rounds, since (because of their nature) they are not that competitive and reach an agreement relatively quickly. The remaining profiles 7, 8 and 9 are based on 150 rounds of data.
\\\\
What can be seen if these results are compared with our actual preference profiles, is that they do give an indication what the main preference of the agent is, but for the other preferences the estimation can be very off. This result can be explained by the agent only negotiation with a clone of itself, which both do not bid based on the issues, but based on the utilities. We are sure that, when faced with a different opponent that adjusts its issues, the estimations will be more accurate.